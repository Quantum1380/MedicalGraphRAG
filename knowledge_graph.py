from typing import Optional
from tqdm import tqdm
import numpy as np
import json
import os
import re
import time
import pickle
from config import config
from neo4j import GraphDatabase, basic_auth
from string import Template
from typing import Dict, List, Any
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from pyvis.network import Network


class KnowledgeGraphManager:
    def __init__(self, ann_leaf_size: int = 30):
        self.vector_index_file = config.KNOWLEDGE_INDEX
        self.embedding_batch_size = 20
        self.similarity_threshold = 0.7
        self.top_k = 5
        self.ann_leaf_size = ann_leaf_size
        self.entity_cache, self.embeddings_cache = {}, {}
        self.vector_index = self._init_vector_index()
        self.driver = self._init_graph_db()
        self.kg_schema = self._load_kg_schema()
        self.openai_client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY", config.TONGYI_KEY),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        self.ann_models = {"entities": None, "relationships": None}
        if self.driver and self.is_first_run:
            self._create_vector_index()
            self.build_vector_index_from_neo4j()
            self.build_ann_models()
            self._save_vector_index()
        elif self.driver and not self.is_first_run:
            self.build_ann_models()

    def _init_vector_index(self) -> Dict:
        """
        åˆå§‹åŒ–å‘é‡ç´¢å¼•ç»“æ„
        å°è¯•ä»æ–‡ä»¶åŠ è½½ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çš„
        """
        # é»˜è®¤å‘é‡ç´¢å¼•ç»“æ„
        default_index = {
            "entities": {
                "ids": [],  # å®ä½“IDåˆ—è¡¨
                "names": [],  # å®ä½“åç§°åˆ—è¡¨
                "types": [],  # å®ä½“ç±»å‹åˆ—è¡¨
                "embeddings": np.empty((0, 1536))  # å®ä½“åµŒå…¥å‘é‡çŸ©é˜µ
            },
            "relationships": {
                "ids": [],  # å…³ç³»IDåˆ—è¡¨
                "types": [],  # å…³ç³»ç±»å‹åˆ—è¡¨
                "sources": [],  # æºå®ä½“åç§°åˆ—è¡¨
                "targets": [],  # ç›®æ ‡å®ä½“åç§°åˆ—è¡¨
                "embeddings": np.empty((0, 1536))  # å…³ç³»åµŒå…¥å‘é‡çŸ©é˜µ
            }
        }

        # æ ‡è®°æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆåˆå§‹å€¼ä¸ºTrueï¼‰
        self.is_first_run = True

        try:
            # æ£€æŸ¥å‘é‡ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(self.vector_index_file):
                print(f"ğŸ” æ‰¾åˆ°å‘é‡ç´¢å¼•æ–‡ä»¶: {self.vector_index_file}")
                with open(self.vector_index_file, "rb") as f:
                    index_data = pickle.load(f)

                    # æ£€æŸ¥ç´¢å¼•ç»“æ„æ˜¯å¦æœ‰æ•ˆ
                    if ("entities" in index_data and "relationships" in index_data and
                            "ids" in index_data["entities"] and "embeddings" in index_data["entities"] and
                            "ids" in index_data["relationships"] and "embeddings" in index_data["relationships"]):
                        print(f"âœ… æˆåŠŸåŠ è½½å‘é‡ç´¢å¼•")
                        self.is_first_run = False  # æ ‡è®°ä¸ºéé¦–æ¬¡è¿è¡Œ
                        return index_data
                    else:
                        print("âš ï¸ ç´¢å¼•æ–‡ä»¶ç»“æ„æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤ç´¢å¼•")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å‘é‡ç´¢å¼•æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç´¢å¼•")
        except Exception as e:
            print(f"âŒ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç´¢å¼•")

        # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤ç´¢å¼•
        return default_index

    def _save_vector_index(self):
        """ä¿å­˜å‘é‡ç´¢å¼•åˆ°æ–‡ä»¶"""
        try:
            with open(self.vector_index_file, "wb") as f:
                pickle.dump(self.vector_index, f)
            print(f"ğŸ’¾ å‘é‡ç´¢å¼•å·²ä¿å­˜è‡³: {self.vector_index_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")

    def _init_graph_db(self) -> Optional[Any]:
        """
        åˆå§‹åŒ–å›¾æ•°æ®åº“è¿æ¥

        è¿”å›:
            Neo4jé©±åŠ¨å¯¹è±¡æˆ–Noneï¼ˆå¦‚æœè¿æ¥å¤±è´¥ï¼‰
        """
        try:
            # åˆ›å»ºæ•°æ®åº“é©±åŠ¨
            driver = GraphDatabase.driver(
                config.NEO4J_URI,  # æ•°æ®åº“åœ°å€
                auth=basic_auth(config.NEO4J_USER, config.NEO4J_PASSWORD)  # è®¤è¯ä¿¡æ¯
            )

            # æµ‹è¯•è¿æ¥æ˜¯å¦æˆåŠŸ
            try:
                with driver.session() as session:
                    # è¿è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
                    result = session.run("RETURN 'connection_test' AS test")
                    record = result.single()  # è·å–ç¬¬ä¸€æ¡è®°å½•
                    result.consume()  # æ˜¾å¼æ¶ˆè´¹ç»“æœé›†ï¼ˆé‡Šæ”¾èµ„æºï¼‰

                    # æ£€æŸ¥æµ‹è¯•ç»“æœ
                    if record and record["test"] == "connection_test":
                        print("âœ… Neo4jè¿æ¥æˆåŠŸ")
                        return driver
            except Exception as test_e:
                print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {str(test_e)}")

            print("âŒ Neo4jè¿æ¥æµ‹è¯•å¤±è´¥")
            return None
        except Exception as e:
            print(f"âŒ å›¾æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return None

    def _load_kg_schema(self) -> Dict:
        """
        åŠ è½½çŸ¥è¯†å›¾è°±æ¨¡å¼é…ç½®

        è¿”å›:
            çŸ¥è¯†å›¾è°±æ¨¡å¼å­—å…¸
        """
        schema_path = config.KG_SCHEMA  # æ¨¡å¼é…ç½®æ–‡ä»¶è·¯å¾„
        default_schema = {
            "name": "é€šç”¨çŸ¥è¯†å›¾è°±",
            "description": "é»˜è®¤çŸ¥è¯†å›¾è°±æ¨¡å¼ï¼Œæ”¯æŒå¤šç§å®ä½“ç±»å‹å’Œå…³ç³»",
            "entity_types": [
                {"name": "äººç‰©", "properties": ["å§“å", "èŒä¸š", "å›½ç±", "å‡ºç”Ÿæ—¥æœŸ"]},
                {"name": "ç»„ç»‡", "properties": ["åç§°", "ç±»å‹", "æˆç«‹æ—¶é—´", "åˆ›å§‹äºº"]},
                {"name": "åœ°ç‚¹", "properties": ["åç§°", "ç±»å‹", "æ‰€å±å›½å®¶", "åæ ‡"]},
                {"name": "äº‹ä»¶", "properties": ["åç§°", "æ—¶é—´", "åœ°ç‚¹", "å‚ä¸è€…"]},
                {"name": "æ¦‚å¿µ", "properties": ["åç§°", "å®šä¹‰", "ç›¸å…³é¢†åŸŸ"]},
                {"name": "æŠ€æœ¯", "properties": ["åç§°", "åº”ç”¨é¢†åŸŸ", "å‘æ˜è€…", "å‘æ˜æ—¶é—´"]}
            ],
            "relationship_types": [
                {"name": "å±äº", "source": ["äººç‰©", "ç»„ç»‡"], "target": ["ç»„ç»‡"]},
                {"name": "ä½äº", "source": ["äººç‰©", "ç»„ç»‡", "äº‹ä»¶"], "target": ["åœ°ç‚¹"]},
                {"name": "å‚ä¸", "source": ["äººç‰©", "ç»„ç»‡"], "target": ["äº‹ä»¶"]},
                {"name": "å‘æ˜", "source": ["äººç‰©"], "target": ["æŠ€æœ¯"]},
                {"name": "åº”ç”¨", "source": ["æŠ€æœ¯"], "target": ["é¢†åŸŸ"]},
                {"name": "ç›¸å…³", "source": ["æ¦‚å¿µ", "æŠ€æœ¯"], "target": ["æ¦‚å¿µ", "æŠ€æœ¯"]},
                {"name": "åŒ…å«", "source": ["æ¦‚å¿µ"], "target": ["æ¦‚å¿µ"]},
                {"name": "å‘ç”Ÿäº", "source": ["äº‹ä»¶"], "target": ["æ—¶é—´"]},
                {"name": "é¢†å¯¼", "source": ["äººç‰©"], "target": ["ç»„ç»‡"]},
                {"name": "åˆä½œ", "source": ["äººç‰©", "ç»„ç»‡"], "target": ["äººç‰©", "ç»„ç»‡"]}
            ],
            "extraction_prompt": Template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±å·¥ç¨‹å¸ˆï¼Œè¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ï¼š
1. åªæå–æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°çš„å®ä½“å’Œå…³ç³»
2. ä½¿ç”¨JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä¸¤ä¸ªåˆ—è¡¨ï¼š"entities"å’Œ"relationships"
3. å®ä½“æ ¼å¼ï¼š{"id": "å”¯ä¸€ID", "name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "properties": {"å±æ€§å": "å±æ€§å€¼"}}
4. å…³ç³»æ ¼å¼ï¼š{"source": "æºå®ä½“ID", "target": "ç›®æ ‡å®ä½“ID", "type": "å…³ç³»ç±»å‹", "properties": {"å±æ€§å": "å±æ€§å€¼"}}
5. å®ä½“ç±»å‹å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š{entity_types}
6. å…³ç³»ç±»å‹å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š{relationship_types}
æ–‡æœ¬å†…å®¹ï¼š
{text}
""")
        }

        try:
            # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(schema_path):
                with open(schema_path, "r", encoding="utf-8") as f:
                    schema_data = json.load(f)

                    # å¤„ç†æå–æç¤ºæ¨¡æ¿
                    if "extraction_prompt" in schema_data:
                        schema_data["extraction_prompt"] = Template(schema_data["extraction_prompt"])

                    return schema_data
            else:
                # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
                with open(schema_path, "w", encoding="utf-8") as f:
                    save_data = default_schema.copy()
                    # å°†æ¨¡æ¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿ä¿å­˜
                    save_data["extraction_prompt"] = save_data["extraction_prompt"].template
                    json.dump(save_data, f, ensure_ascii=False, indent=2)

                print(f"âœ… åˆ›å»ºé»˜è®¤çŸ¥è¯†å›¾è°±æ¨¡å¼: {schema_path}")
                return default_schema
        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†å›¾è°±æ¨¡å¼å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤æ¨¡å¼")
            return default_schema

    def update_kg_schema(self, new_schema: Dict) -> bool:
        """
        æ›´æ–°çŸ¥è¯†å›¾è°±æ¨¡å¼é…ç½®

        å‚æ•°:
            new_schema: æ–°çš„æ¨¡å¼é…ç½®å­—å…¸

        è¿”å›:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            save_data = new_schema.copy()

            # å¤„ç†æå–æç¤ºæ¨¡æ¿
            if isinstance(save_data["extraction_prompt"], Template):
                save_data["extraction_prompt"] = save_data["extraction_prompt"].template

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open("kg_schema.json", "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)

            # é‡æ–°åŠ è½½æ¨¡å¼
            self.kg_schema = self._load_kg_schema()
            print("âœ… çŸ¥è¯†å›¾è°±æ¨¡å¼æ›´æ–°æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æ›´æ–°çŸ¥è¯†å›¾è°±æ¨¡å¼å¤±è´¥: {e}")
            return False

    def call_openai_api(self, prompt: str) -> Dict:
        """
        è°ƒç”¨å¤§æ¨¡å‹APIæå–å®ä½“å’Œå…³ç³»

        å‚æ•°:
            prompt: æç¤ºè¯æ–‡æœ¬

        è¿”å›:
            åŒ…å«å®ä½“å’Œå…³ç³»çš„å­—å…¸
        """
        try:
            # è°ƒç”¨å¤§æ¨¡å‹API
            response = self.openai_client.chat.completions.create(
                model="qwen-plus",  # ä½¿ç”¨qwen-plusæ¨¡å‹
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±å·¥ç¨‹å¸ˆ"},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "text"}  # ç¡®ä¿è¿”å›çº¯æ–‡æœ¬
            )

            # è·å–æ¨¡å‹è¿”å›çš„å†…å®¹
            content = response.choices[0].message.content

            # å°è¯•è§£æJSON
            try:
                # æŸ¥æ‰¾JSONéƒ¨åˆ†ï¼ˆæ¨¡å‹è¿”å›å¯èƒ½åŒ…å«éJSONå†…å®¹ï¼‰
                json_start = content.find('{')
                json_end = content.rfind('}') + 1

                # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æœ‰æ•ˆçš„JSON
                if json_start == -1 or json_end == 0:
                    print(f"âŒ æœªæ‰¾åˆ°JSONå†…å®¹: {content[:200]}...")
                    return {"entities": [], "relationships": []}

                json_str = content[json_start:json_end]

                # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                json_str = re.sub(r',\s*]', ']', json_str)  # ä¿®å¤å¤šä½™çš„é€—å·
                json_str = re.sub(r',\s*}', '}', json_str)  # ä¿®å¤å¤šä½™çš„é€—å·
                json_str = re.sub(r"(\w+):", r'"\1":', json_str)  # ä¸ºé”®æ·»åŠ å¼•å·

                # è§£æJSON
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"åŸå§‹å†…å®¹: {content[:200]}...")
                return {"entities": [], "relationships": []}
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return {"entities": [], "relationships": []}

    def extract_entities_relations(self, text: str) -> Dict:
        """
        ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›:
            åŒ…å«å®ä½“å’Œå…³ç³»çš„å­—å…¸
        """
        # è·å–å®ä½“ç±»å‹åˆ—è¡¨
        entity_types = ", ".join([et["name"] for et in self.kg_schema["entity_types"]])
        # è·å–å…³ç³»ç±»å‹åˆ—è¡¨
        relationship_types = ", ".join([rt["name"] for rt in self.kg_schema["relationship_types"]])

        # è·å–æå–æç¤ºæ¨¡æ¿
        if hasattr(self.kg_schema["extraction_prompt"], 'template'):
            raw_prompt_str = self.kg_schema["extraction_prompt"].template
        else:
            raw_prompt_str = self.kg_schema["extraction_prompt"]

        # å¡«å……æ¨¡æ¿
        prompt = raw_prompt_str
        prompt = prompt.replace("{entity_types}", entity_types)
        prompt = prompt.replace("{relationship_types}", relationship_types)
        prompt = prompt.replace("{text}", text)

        print("ğŸ” ä½¿ç”¨å¤§æ¨¡å‹æå–å®ä½“å…³ç³»...")
        # è°ƒç”¨APIæå–å®ä½“å…³ç³»
        result = self.call_openai_api(prompt)

        print(f"âœ… æå–åˆ° {len(result.get('entities', []))} ä¸ªå®ä½“")
        print(f"âœ… æå–åˆ° {len(result.get('relationships', []))} ä¸ªå…³ç³»")
        return result

    def generate_embedding(self, text: str) -> List[float]:
        """
        ä¸ºå•ä¸ªæ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›:
            åµŒå…¥å‘é‡åˆ—è¡¨ï¼ˆ1536ç»´ï¼‰
        """
        if not text:
            return []

        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥æ–‡æœ¬çš„åµŒå…¥
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]

        try:
            # è°ƒç”¨åµŒå…¥æ¨¡å‹API
            response = self.openai_client.embeddings.create(
                model="text-embedding-v1",  # ä½¿ç”¨é€šä¹‰åƒé—®æ–‡æœ¬åµŒå…¥æ¨¡å‹
                input=[text]  # è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            )
            # è·å–åµŒå…¥å‘é‡
            embedding = response.data[0].embedding
            # å­˜å…¥ç¼“å­˜
            self.embeddings_cache[text] = embedding
            return embedding
        except Exception as e:
            print(f"âŒ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            return []

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ï¼Œæé«˜æ•ˆç‡

        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨

        è¿”å›:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []

        # æ£€æŸ¥ç¼“å­˜
        embeddings = []
        uncached_texts = []

        # åˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„æ–‡æœ¬
        for text in texts:
            if text in self.embeddings_cache:
                embeddings.append(self.embeddings_cache[text])
            else:
                uncached_texts.append(text)

        # ä¸ºæœªç¼“å­˜çš„æ–‡æœ¬ç”ŸæˆåµŒå…¥
        if uncached_texts:
            try:
                # åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…è¯·æ±‚è¿‡å¤§ï¼‰
                batch_size = self.embedding_batch_size  # ä½¿ç”¨é…ç½®çš„æ‰¹é‡å¤§å°
                for i in range(0, len(uncached_texts), batch_size):
                    batch = uncached_texts[i:i + batch_size]

                    # è°ƒç”¨API
                    response = self.openai_client.embeddings.create(
                        model="text-embedding-v1",
                        input=batch
                    )

                    # å¤„ç†è¿”å›ç»“æœ
                    for j, data in enumerate(response.data):
                        embedding = data.embedding
                        text = batch[j]
                        # å­˜å…¥ç¼“å­˜
                        self.embeddings_cache[text] = embedding
                        embeddings.append(embedding)
            except Exception as e:
                print(f"âŒ æ‰¹é‡ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
                # ä¸ºå¤±è´¥çš„è¯·æ±‚æ·»åŠ ç©ºåµŒå…¥
                for _ in range(len(uncached_texts)):
                    embeddings.append([])

        return embeddings

    def _create_vector_index(self):
        """åœ¨Neo4jæ•°æ®åº“ä¸­åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        if not self.driver:
            return

        try:
            with self.driver.session() as session:
                # åˆ›å»ºå®ä½“å‘é‡ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                session.run("""
                CREATE VECTOR INDEX IF NOT EXISTS FOR (e:Entity) ON e.embedding 
                OPTIONS {indexConfig: {
                    `vector.dimensions`: 1536,
                    `vector.similarity_function`: 'cosine'
                }}
                """)

                # åˆ›å»ºå…³ç³»å‘é‡ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                session.run("""
                CREATE VECTOR INDEX IF NOT EXISTS FOR ()-[r:RELATIONSHIP]-() ON r.embedding 
                OPTIONS {indexConfig: {
                    `vector.dimensions`: 1536,
                    `vector.similarity_function`: 'cosine'
                }}
                """)
            print("âœ… æ•°æ®åº“å‘é‡ç´¢å¼•å·²åˆ›å»ºæˆ–å·²å­˜åœ¨")
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ•°æ®åº“å‘é‡ç´¢å¼•å¤±è´¥: {e}")

    def build_vector_index_from_neo4j(self):
        """ä»Neo4jåŠ è½½æ‰€æœ‰å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºå†…å­˜å‘é‡ç´¢å¼•"""
        if not self.driver:
            print("âŒ å›¾æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•æ„å»ºç´¢å¼•")
            return

        try:
            with self.driver.session() as session:
                print("ğŸ”„ ä»Neo4jåŠ è½½å®ä½“...")
                # æŸ¥è¯¢æ‰€æœ‰å®ä½“
                result = session.run("MATCH (e) RETURN id(e) as id, e.name as name, labels(e) as labels")
                entities = []
                for record in tqdm(result, desc="åŠ è½½å®ä½“", disable=not self.is_first_run):
                    entities.append({
                        "id": record["id"],
                        "name": record["name"],
                        "labels": record["labels"]
                    })

                print("ğŸ§  ç”Ÿæˆå®ä½“åµŒå…¥...")
                # ä¸ºå®ä½“ç”Ÿæˆæ–‡æœ¬æè¿°
                entity_texts = [
                    f"{e['labels'][0] if e['labels'] else 'Entity'}: {e['name']}"
                    for e in entities
                ]
                # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
                entity_embeddings = self.generate_embeddings_batch(entity_texts)

                print("ğŸ“¥ æ›´æ–°å®ä½“ç´¢å¼•...")
                # æ›´æ–°å†…å­˜ç´¢å¼•
                for i, entity in enumerate(entities):
                    embedding = entity_embeddings[i] if i < len(entity_embeddings) else []
                    if embedding:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        if entity["id"] not in self.vector_index["entities"]["ids"]:
                            self.vector_index["entities"]["ids"].append(str(entity["id"]))
                            self.vector_index["entities"]["names"].append(entity["name"])
                            self.vector_index["entities"]["types"].append(
                                entity["labels"][0] if entity["labels"] else "Entity")
                            # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°çŸ©é˜µä¸­
                            self.vector_index["entities"]["embeddings"] = np.vstack((
                                self.vector_index["entities"]["embeddings"],
                                np.array(embedding).reshape(1, -1)
                            ))

                print("ğŸ”„ ä»Neo4jåŠ è½½å…³ç³»...")
                # æŸ¥è¯¢æ‰€æœ‰å…³ç³»
                result = session.run("""
                MATCH ()-[r]->()
                RETURN id(r) as id, type(r) as type, startNode(r).name as source, endNode(r).name as target
                """)
                relationships = []
                for record in tqdm(result, desc="åŠ è½½å…³ç³»", disable=not self.is_first_run):
                    relationships.append({
                        "id": record["id"],
                        "type": record["type"],
                        "source": record["source"],
                        "target": record["target"]
                    })

                print("ğŸ§  ç”Ÿæˆå…³ç³»åµŒå…¥...")
                # ä¸ºå…³ç³»ç”Ÿæˆæ–‡æœ¬æè¿°
                rel_texts = [
                    f"{rel['type']}: {rel['source']} -> {rel['target']}"
                    for rel in relationships
                ]
                # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
                rel_embeddings = self.generate_embeddings_batch(rel_texts)

                print("ğŸ“¥ æ›´æ–°å…³ç³»ç´¢å¼•...")
                # æ›´æ–°å†…å­˜ç´¢å¼•
                for i, rel in enumerate(relationships):
                    embedding = rel_embeddings[i] if i < len(rel_embeddings) else []
                    if embedding:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        if rel["id"] not in self.vector_index["relationships"]["ids"]:
                            self.vector_index["relationships"]["ids"].append(rel["id"])
                            self.vector_index["relationships"]["types"].append(rel["type"])
                            self.vector_index["relationships"]["sources"].append(rel["source"])
                            self.vector_index["relationships"]["targets"].append(rel["target"])
                            # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°çŸ©é˜µä¸­
                            self.vector_index["relationships"]["embeddings"] = np.vstack((
                                self.vector_index["relationships"]["embeddings"],
                                np.array(embedding).reshape(1, -1)
                            ))

                print(
                    f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±åŠ è½½ {len(self.vector_index['entities']['ids'])} ä¸ªå®ä½“å’Œ {len(self.vector_index['relationships']['ids'])} ä¸ªå…³ç³»")
        except Exception as e:
            print(f"âŒ æ„å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")

    def build_ann_models(self):
        """åŠ¨æ€æ„å»º ANN æ¨¡å‹ï¼Œé˜²æ­¢ n_neighbors > n_samples_fit"""
        for key, emb in [
            ("entities", self.vector_index["entities"]["embeddings"]),
            ("relationships", self.vector_index["relationships"]["embeddings"])
        ]:
            if emb.shape[0] == 0:
                self.ann_models[key] = None
                continue

            # ç¡®ä¿ n_neighbors ä¸è¶…è¿‡æ ·æœ¬æ•°
            max_k = max(1, emb.shape[0] - 1)
            k = min(self.top_k * 2, max_k)
            print(f"ğŸ”§ æ„å»º {key} ANN æ¨¡å‹ï¼šæ ·æœ¬æ•°={emb.shape[0]}ï¼Œn_neighbors={k}")
            self.ann_models[key] = NearestNeighbors(n_neighbors=k, metric='cosine')
            self.ann_models[key].fit(emb)

    # ========= æ–°å¢å›¾æŸ¥è¯¢æ–¹æ³• =========
    def shortest_path(self, source: str, target: str) -> List[Dict]:
        if not self.driver:
            return []
        with self.driver.session() as session:
            res = session.run("""
                MATCH path = shortestPath((a:Entity {name:$src})-[*]-(b:Entity {name:$tgt}))
                RETURN [n in nodes(path) | {id:id(n), name:n.name, type:labels(n)[0]}] as nodes,
                       [r in relationships(path) | {source:startNode(r).name, target:endNode(r).name, type:type(r)}] as rels
            """, src=source, tgt=target)
            return [dict(r) for r in res]

    def centrality_analysis(self) -> Dict:
        if not self.driver:
            return {}
        with self.driver.session() as session:
            # å‡ºåº¦ä¸­å¿ƒæ€§
            res = session.run("""
                MATCH (n)-[r]-()
                RETURN n.name as node, count(r) as degree
                ORDER BY degree DESC LIMIT 10
            """)
            return {r["node"]: r["degree"] for r in res}

    def search_nodes(self, keyword: str) -> Dict:
        if not self.driver:
            return {"nodes": [], "links": []}
        with self.driver.session() as session:
            res = session.run("""
                MATCH (n)-[r]-(m)
                WHERE n.name CONTAINS $kw OR m.name CONTAINS $kw
                RETURN n.name as source, labels(n)[0] as source_type,
                       type(r) as relationship,
                       m.name as target, labels(m)[0] as target_type,
                       id(n) as source_id, id(m) as target_id
                LIMIT 50
            """, kw=keyword)
            nodes, links, node_set = [], [], set()
            for rec in res:
                for id_, name, type_ in [(rec["source_id"], rec["source"], rec["source_type"]),
                                         (rec["target_id"], rec["target"], rec["target_type"])]:
                    if id_ not in node_set:
                        nodes.append({"id": id_, "name": name, "type": type_})
                        node_set.add(id_)
                links.append({"source": rec["source_id"], "target": rec["target_id"], "type": rec["relationship"]})
            return {"nodes": nodes, "links": links}

    def save_to_neo4j(self, entities: List[Dict], relationships: List[Dict]) -> bool:
        """
        å°†æå–çš„å®ä½“å…³ç³»ä¿å­˜åˆ°Neo4jæ•°æ®åº“
        å¹¶æ›´æ–°å‘é‡ç´¢å¼•ï¼ˆä»…æ–°å¢éƒ¨åˆ†ï¼‰

        å‚æ•°:
            entities: å®ä½“åˆ—è¡¨
            relationships: å…³ç³»åˆ—è¡¨

        è¿”å›:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not self.driver:
            print("âŒ å›¾æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•ä¿å­˜")
            return False

        try:
            with self.driver.session() as session:
                # åˆ›å»ºå®ä½“
                new_entities = []  # å­˜å‚¨æ–°æ·»åŠ çš„å®ä½“

                for entity in entities:
                    # è·å–å®ä½“ä¿¡æ¯
                    entity_name = entity["name"]
                    entity_type = entity["type"]
                    properties = entity.get("properties", {})

                    # ç”Ÿæˆå®ä½“æè¿°æ–‡æœ¬ï¼ˆç”¨äºåµŒå…¥ï¼‰
                    entity_text = f"{entity_type}: {entity_name}"

                    # åˆ›å»ºæˆ–åˆå¹¶å®ä½“èŠ‚ç‚¹
                    query = """
                    MERGE (e:Entity:%s {name: $name})
                    SET e += $props
                    RETURN id(e) as id
                    """ % entity_type

                    # æ‰§è¡ŒæŸ¥è¯¢
                    result = session.run(query, name=entity_name, props=properties)
                    record = result.single()
                    result.consume()

                    if record:
                        entity_id = record["id"]

                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å®ä½“
                        if entity_id not in self.vector_index["entities"]["ids"]:
                            # ç”ŸæˆåµŒå…¥å‘é‡
                            embedding = self.generate_embedding(entity_text)

                            # æ›´æ–°èŠ‚ç‚¹åµŒå…¥
                            session.run("""
                            MATCH (e) WHERE id(e) = $id
                            SET e.embedding = $embedding
                            """, id=entity_id, embedding=embedding)

                            # æ·»åŠ åˆ°ç¼“å­˜å’Œæ–°å®ä½“åˆ—è¡¨
                            self.entity_cache[entity_id] = {
                                "name": entity_name,
                                "type": entity_type,
                                "embedding": embedding
                            }

                            new_entities.append({
                                "id": entity_id,
                                "name": entity_name,
                                "type": entity_type,
                                "embedding": embedding
                            })

                # åˆ›å»ºå…³ç³»
                new_relationships = []  # å­˜å‚¨æ–°æ·»åŠ çš„å…³ç³»

                for rel in relationships:
                    # è·å–å…³ç³»ä¿¡æ¯
                    source_id = rel["source"]
                    target_id = rel["target"]
                    rel_type = rel["type"]
                    properties = rel.get("properties", {})

                    # è·å–å®ä½“åç§°
                    source_name = self.entity_cache.get(source_id, {}).get("name", "Unknown")
                    target_name = self.entity_cache.get(target_id, {}).get("name", "Unknown")

                    # ç”Ÿæˆå…³ç³»æè¿°æ–‡æœ¬ï¼ˆç”¨äºåµŒå…¥ï¼‰
                    rel_text = f"{rel_type}: {source_name} -> {target_name}"

                    # åˆ›å»ºå…³ç³»
                    query = """
                    MATCH (source), (target) 
                    WHERE id(source) = $source_id AND id(target) = $target_id
                    MERGE (source)-[r:%s]->(target)
                    SET r.type = $rel_type
                    SET r += $props
                    RETURN id(r) as id
                    """ % rel_type

                    # æ‰§è¡ŒæŸ¥è¯¢
                    result = session.run(query,
                                         source_id=source_id,
                                         target_id=target_id,
                                         rel_type=rel_type,
                                         props=properties)
                    record = result.single()
                    result.consume()

                    if record:
                        rel_id = record["id"]

                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å…³ç³»
                        if rel_id not in self.vector_index["relationships"]["ids"]:
                            # ç”ŸæˆåµŒå…¥å‘é‡
                            rel_embedding = self.generate_embedding(rel_text)

                            # æ›´æ–°å…³ç³»åµŒå…¥
                            session.run("""
                            MATCH ()-[r]->() WHERE id(r) = $id
                            SET r.embedding = $embedding
                            """, id=rel_id, embedding=rel_embedding)

                            new_relationships.append({
                                "id": rel_id,
                                "type": rel_type,
                                "source": source_name,
                                "target": target_name,
                                "embedding": rel_embedding
                            })

                # æ›´æ–°å†…å­˜ç´¢å¼•
                for entity in new_entities:
                    self.vector_index["entities"]["ids"].append(entity["id"])
                    self.vector_index["entities"]["names"].append(entity["name"])
                    self.vector_index["entities"]["types"].append(entity["type"])
                    self.vector_index["entities"]["embeddings"] = np.vstack((
                        self.vector_index["entities"]["embeddings"],
                        np.array(entity["embedding"]).reshape(1, -1)
                    ))

                for rel in new_relationships:
                    self.vector_index["relationships"]["ids"].append(rel["id"])
                    self.vector_index["relationships"]["types"].append(rel["type"])
                    self.vector_index["relationships"]["sources"].append(rel["source"])
                    self.vector_index["relationships"]["targets"].append(rel["target"])
                    self.vector_index["relationships"]["embeddings"] = np.vstack((
                        self.vector_index["relationships"]["embeddings"],
                        np.array(rel["embedding"]).reshape(1, -1)
                    ))

                print(f"âœ… æˆåŠŸä¿å­˜ {len(entities)} ä¸ªå®ä½“å’Œ {len(relationships)} ä¸ªå…³ç³»åˆ°Neo4j")
                print(f"ğŸ“¥ æ›´æ–°ç´¢å¼•: æ–°å¢ {len(new_entities)} ä¸ªå®ä½“, {len(new_relationships)} ä¸ªå…³ç³»")

                # é‡å»ºANNæ¨¡å‹
                self.build_ann_models()

                # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
                self._save_vector_index()

                return True
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ°Neo4jå¤±è´¥: {e}")
            return False

    def find_similar_entities_batch(self, embeddings: np.ndarray, threshold: float = 0.75, top_k: int = 5) -> List[
        List[Dict]]:
        """
        æ‰¹é‡æŸ¥æ‰¾ç›¸ä¼¼çš„å®ä½“ï¼ˆä½¿ç”¨ANNåŠ é€Ÿï¼‰

        å‚æ•°:
            embeddings: æŸ¥è¯¢åµŒå…¥å‘é‡çŸ©é˜µ
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼ç»“æœæ•°é‡

        è¿”å›:
            æ¯ä¸ªæŸ¥è¯¢åµŒå…¥å¯¹åº”çš„ç›¸ä¼¼å®ä½“åˆ—è¡¨
        """
        results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if embeddings.size == 0 or self.vector_index["entities"]["embeddings"].size == 0:
            return [[] for _ in range(len(embeddings))]

        # ä½¿ç”¨ANNæ¨¡å‹è¿›è¡Œè¿‘ä¼¼æœç´¢
        if self.ann_models["entities"]:
            # æŸ¥æ‰¾æœ€è¿‘çš„é‚»å±…
            distances, indices = self.ann_models["entities"].kneighbors(embeddings, n_neighbors=top_k * 2)

            for i, query_embedding in enumerate(embeddings):
                similar_entities = []
                # è·å–å€™é€‰å®ä½“
                for j, idx in enumerate(indices[i]):
                    # ä½™å¼¦è·ç¦»è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = 1 - distances[i][j]

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                    if similarity < threshold:
                        continue

                    # è·å–å®ä½“ID
                    entity_id = self.vector_index["entities"]["ids"][idx]

                    similar_entities.append({
                        "id": entity_id,
                        "name": self.vector_index["entities"]["names"][idx],
                        "type": self.vector_index["entities"]["types"][idx],
                        "similarity": similarity
                    })

                # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶æˆªå–top_k
                similar_entities.sort(key=lambda x: x["similarity"], reverse=True)
                results.append(similar_entities[:top_k])
        else:
            # å¦‚æœæ²¡æœ‰ANNæ¨¡å‹ï¼Œä½¿ç”¨æš´åŠ›æœç´¢ï¼ˆä¸æ¨èç”¨äºå¤§æ•°æ®é›†ï¼‰
            db_embeddings = self.vector_index["entities"]["embeddings"]
            for query_embedding in embeddings:
                # è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
                similarities = cosine_similarity([query_embedding], db_embeddings)[0]

                # è·å–è¶…è¿‡é˜ˆå€¼çš„ç›¸ä¼¼å®ä½“
                similar_entities = []
                for idx, sim in enumerate(similarities):
                    if sim >= threshold:
                        similar_entities.append({
                            "id": self.vector_index["entities"]["ids"][idx],
                            "name": self.vector_index["entities"]["names"][idx],
                            "type": self.vector_index["entities"]["types"][idx],
                            "similarity": sim
                        })

                # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶æˆªå–top_k
                similar_entities.sort(key=lambda x: x["similarity"], reverse=True)
                results.append(similar_entities[:top_k])

        return results

    def find_similar_relationships_batch(self, embeddings: np.ndarray, threshold: float = 0.7, top_k: int = 5) -> List[
        List[Dict]]:
        """
        æ‰¹é‡æŸ¥æ‰¾ç›¸ä¼¼çš„å…³ç³»ï¼ˆä½¿ç”¨ANNåŠ é€Ÿï¼‰

        å‚æ•°:
            embeddings: æŸ¥è¯¢åµŒå…¥å‘é‡çŸ©é˜µ
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼ç»“æœæ•°é‡

        è¿”å›:
            æ¯ä¸ªæŸ¥è¯¢åµŒå…¥å¯¹åº”çš„ç›¸ä¼¼å…³ç³»åˆ—è¡¨
        """
        results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if embeddings.size == 0 or self.vector_index["relationships"]["embeddings"].size == 0:
            return [[] for _ in range(len(embeddings))]

        # ä½¿ç”¨ANNæ¨¡å‹è¿›è¡Œè¿‘ä¼¼æœç´¢
        if self.ann_models["relationships"]:
            # æŸ¥æ‰¾æœ€è¿‘çš„é‚»å±…
            distances, indices = self.ann_models["relationships"].kneighbors(embeddings, n_neighbors=top_k * 2)

            for i, query_embedding in enumerate(embeddings):
                similar_rels = []
                # è·å–å€™é€‰å…³ç³»
                for j, idx in enumerate(indices[i]):
                    # ä½™å¼¦è·ç¦»è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = 1 - distances[i][j]

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                    if similarity < threshold:
                        continue

                    # è·å–å…³ç³»ID
                    rel_id = self.vector_index["relationships"]["ids"][idx]

                    similar_rels.append({
                        "id": rel_id,
                        "type": self.vector_index["relationships"]["types"][idx],
                        "source": self.vector_index["relationships"]["sources"][idx],
                        "target": self.vector_index["relationships"]["targets"][idx],
                        "similarity": similarity
                    })

                # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶æˆªå–top_k
                similar_rels.sort(key=lambda x: x["similarity"], reverse=True)
                results.append(similar_rels[:top_k])
        else:
            # æš´åŠ›æœç´¢
            db_embeddings = self.vector_index["relationships"]["embeddings"]
            for query_embedding in embeddings:
                # è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
                similarities = cosine_similarity([query_embedding], db_embeddings)[0]

                # è·å–è¶…è¿‡é˜ˆå€¼çš„ç›¸ä¼¼å…³ç³»
                similar_rels = []
                for idx, sim in enumerate(similarities):
                    if sim >= threshold:
                        similar_rels.append({
                            "id": self.vector_index["relationships"]["ids"][idx],
                            "type": self.vector_index["relationships"]["types"][idx],
                            "source": self.vector_index["relationships"]["sources"][idx],
                            "target": self.vector_index["relationships"]["targets"][idx],
                            "similarity": sim
                        })

                # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶æˆªå–top_k
                similar_rels.sort(key=lambda x: x["similarity"], reverse=True)
                results.append(similar_rels[:top_k])

        return results

    def query_kg_by_entities(self, entity_ids: List[int], depth: int = 2) -> List[Dict]:
        """
        åœ¨çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢ä¸å®ä½“ç›¸å…³çš„å­å›¾

        å‚æ•°:
            entity_ids: å®ä½“IDåˆ—è¡¨
            depth: æŸ¥è¯¢æ·±åº¦ï¼ˆå…³ç³»è·³æ•°ï¼‰

        è¿”å›:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        if not self.driver or not entity_ids:
            return []

        try:
            with self.driver.session() as session:
                # æ„å»ºæŸ¥è¯¢è¯­å¥
                query = """
                MATCH path = (start)-[rel*..%d]-(end)
                WHERE id(start) IN $entity_ids
                WITH nodes(path) AS nodes, relationships(path) AS rels
                UNWIND nodes AS node
                UNWIND rels AS rel

                WITH DISTINCT rel, startNode(rel) AS start, endNode(rel) AS end
                RETURN start.name AS source, 
                       labels(start)[0] AS source_type,
                       type(rel) AS relationship, 
                       end.name AS target,
                       labels(end)[0] AS target_type,
                       properties(rel) AS rel_properties
                ORDER BY source, relationship, target
                LIMIT 100
                """ % depth

                # æ‰§è¡ŒæŸ¥è¯¢
                result = session.run(query, entity_ids=entity_ids)
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                records = [dict(record) for record in result]
                result.consume()  # é‡Šæ”¾èµ„æº
                return records
        except Exception as e:
            print(f"âŒ å›¾æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []

    def process_user_query(self, text: str, save_to_db: bool = False,
                           depth: int = 2, similarity_threshold: float = 0.7,
                           top_k: int = 5) -> List[Dict]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»è¦æµç¨‹
        ä¼˜åŒ–ï¼šåªä¸ºæ–°æå–çš„å®ä½“å…³ç³»ç”ŸæˆåµŒå…¥å‘é‡

        å‚æ•°:
            text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            save_to_db: æ˜¯å¦å°†æå–ç»“æœä¿å­˜åˆ°æ•°æ®åº“
            depth: çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ·±åº¦
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼ç»“æœæ•°é‡

        è¿”å›:
            çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()
        print("\n" + "=" * 50)
        print(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {text[:50]}...")
        print("=" * 50)

        # 1. ä»æ–‡æœ¬ä¸­æå–å®ä½“å…³ç³»
        extract_start = time.time()
        extraction_result = self.extract_entities_relations(text)
        entities = extraction_result.get("entities", [])
        relationships = extraction_result.get("relationships", [])
        print(f"â±ï¸ æå–å®ä½“å…³ç³»è€—æ—¶: {time.time() - extract_start:.2f}s")
        print(f"ğŸ“Š æå–ç»“æœ: {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")

        # 2. æŒ‰éœ€ä¿å­˜åˆ°Neo4j
        if save_to_db and self.driver and (entities or relationships):
            save_start = time.time()
            print("ğŸ’¾ ä¿å­˜æå–ç»“æœåˆ°Neo4j...")
            self.save_to_neo4j(entities, relationships)
            print(f"â±ï¸ ä¿å­˜åˆ°æ•°æ®åº“è€—æ—¶: {time.time() - save_start:.2f}s")
        elif save_to_db and not self.driver:
            print("âš ï¸ æ— æ³•ä¿å­˜åˆ°æ•°æ®åº“ï¼šæ•°æ®åº“æœªè¿æ¥")

        # 3. ä¸ºæå–çš„å®ä½“å’Œå…³ç³»ç”ŸæˆåµŒå…¥å‘é‡
        embed_start = time.time()

        # ç”Ÿæˆå®ä½“åµŒå…¥
        entity_texts = [
            f"{entity.get('type', 'å®ä½“')}: {entity['name']}"
            for entity in entities
        ]
        entity_embeddings = self.generate_embeddings_batch(entity_texts)

        # ç”Ÿæˆå…³ç³»åµŒå…¥
        rel_texts = []
        for rel in relationships:
            # æŸ¥æ‰¾æºå®ä½“å’Œç›®æ ‡å®ä½“çš„åç§°
            source_name = next((e["name"] for e in entities if e.get("id") == rel["source"]), "Unknown")
            target_name = next((e["name"] for e in entities if e.get("id") == rel["target"]), "Unknown")
            rel_texts.append(f"{rel['type']}: {source_name} -> {target_name}")

        rel_embeddings = self.generate_embeddings_batch(rel_texts)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿‡æ»¤ç©ºåµŒå…¥
        entity_embeddings_np = np.array([e for e in entity_embeddings if e])
        rel_embeddings_np = np.array([r for r in rel_embeddings if r])

        print(f"â±ï¸ ç”ŸæˆåµŒå…¥å‘é‡è€—æ—¶: {time.time() - embed_start:.2f}s")
        print(f"ğŸ”¢ ç”Ÿæˆå®ä½“åµŒå…¥: {len(entity_embeddings_np)}ä¸ª, å…³ç³»åµŒå…¥: {len(rel_embeddings_np)}ä¸ª")

        # 4. æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å®ä½“å…³ç³»
        similarity_start = time.time()

        # æŸ¥æ‰¾ç›¸ä¼¼å®ä½“
        similar_entities_results = self.find_similar_entities_batch(
            entity_embeddings_np,
            threshold=similarity_threshold,
            top_k=top_k
        )

        # æŸ¥æ‰¾ç›¸ä¼¼å…³ç³»
        similar_rels_results = self.find_similar_relationships_batch(
            rel_embeddings_np,
            threshold=similarity_threshold,
            top_k=top_k
        )

        # æ”¶é›†æ‰€æœ‰ç›¸ä¼¼å®ä½“çš„ID
        all_similar_entity_ids = set()

        # ä»ç›¸ä¼¼å®ä½“ç»“æœä¸­æ”¶é›†ID
        for entity_list in similar_entities_results:
            for entity in entity_list:
                all_similar_entity_ids.add(entity["id"])

        # ä»ç›¸ä¼¼å…³ç³»ç»“æœä¸­æ”¶é›†ç›¸å…³å®ä½“ID
        for rel_list in similar_rels_results:
            for rel in rel_list:
                # æŸ¥æ‰¾å…³ç³»å¯¹åº”çš„æºå®ä½“å’Œç›®æ ‡å®ä½“
                source_id = next((idx for idx, name in enumerate(self.vector_index["entities"]["names"])
                                  if name == rel["source"]), None)
                target_id = next((idx for idx, name in enumerate(self.vector_index["entities"]["names"])
                                  if name == rel["target"]), None)

                if source_id is not None:
                    all_similar_entity_ids.add(self.vector_index["entities"]["ids"][source_id])
                if target_id is not None:
                    all_similar_entity_ids.add(self.vector_index["entities"]["ids"][target_id])

        print(f"â±ï¸ ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶: {time.time() - similarity_start:.2f}s")
        print(f"ğŸ” æ‰¾åˆ° {len(all_similar_entity_ids)} ä¸ªç›¸ä¼¼å®ä½“")

        # 5. åœ¨æ•°æ®åº“ä¸­è¿›è¡Œå¤šè·³æŸ¥è¯¢
        query_start = time.time()
        if all_similar_entity_ids:
            kg_results = self.query_kg_by_entities(list(all_similar_entity_ids), depth)
        else:
            kg_results = []

        print(f"â±ï¸ å›¾æ•°æ®åº“æŸ¥è¯¢è€—æ—¶: {time.time() - query_start:.2f}s")
        print(f"â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.2f}s")
        print(f"âœ… æ‰¾åˆ° {len(kg_results)} æ¡ç›¸å…³å…³ç³»")
        return kg_results

    def format_kg_results(self, records: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–çŸ¥è¯†å›¾è°±ç»“æœä¸ºè‡ªç„¶è¯­è¨€æè¿°

        å‚æ•°:
            records: çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ

        è¿”å›:
            è‡ªç„¶è¯­è¨€æè¿°å­—ç¬¦ä¸²
        """
        if not records:
            return "çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        # æŒ‰å…³ç³»ç±»å‹åˆ†ç»„
        relation_groups = {}
        for record in records:
            rel_type = record["relationship"]
            if rel_type not in relation_groups:
                relation_groups[rel_type] = []

            # æ·»åŠ å±æ€§æè¿°ï¼ˆæ’é™¤åµŒå…¥å‘é‡ï¼‰
            properties = {
                k: v for k, v in record.get("rel_properties", {}).items()
                if k != "embedding" and not k.startswith("vector")
            }

            prop_desc = ""
            if properties:
                prop_desc = " (" + ", ".join([f"{k}: {v}" for k, v in properties.items()]) + ")"

            # æ„å»ºå…³ç³»æè¿°
            relation_desc = f"{record['source']}({record['source_type']}) -> {record['target']}({record['target_type']}){prop_desc}"
            relation_groups[rel_type].append(relation_desc)

        # æ„å»ºè‡ªç„¶è¯­è¨€æè¿°
        descriptions = []
        for rel_type, items in relation_groups.items():
            if len(items) == 1:
                descriptions.append(f"{items[0]} ä¹‹é—´å­˜åœ¨ {rel_type} å…³ç³»ã€‚")
            else:
                # æå–æ‰€æœ‰æºå®ä½“
                sources = set([item.split('->')[0].strip() for item in items])
                # æå–æ‰€æœ‰ç›®æ ‡å®ä½“
                targets = set([item.split('->')[1].strip() for item in items])

                source_list = ", ".join(sources)
                target_list = ", ".join(targets)
                descriptions.append(
                    f"{source_list} ä¸ {target_list} ä¹‹é—´å­˜åœ¨ {rel_type} å…³ç³»ã€‚"
                )

        return "\n".join(descriptions)

    def visualize_kg(self, records: List[Dict], output_file: str = "kg_visualization.html") -> str:
        """
        ç”ŸæˆçŸ¥è¯†å›¾è°±å¯è§†åŒ–

        å‚æ•°:
            records: çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶å

        è¿”å›:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not records:
            print("âš ï¸ æ— æ•°æ®å¯å¯è§†åŒ–")
            return ""

        try:
            # åˆ›å»ºç½‘ç»œå›¾
            net = Network(height="800px", width="100%", notebook=False, directed=True)

            # é¢œè‰²æ˜ å°„ - ä¸ºä¸åŒç±»å‹çš„å®ä½“åˆ†é…ä¸åŒé¢œè‰²
            type_colors = {
                "äººç‰©": "#FF9AA2", "ç»„ç»‡": "#FFB7B2", "åœ°ç‚¹": "#FFDAC1",
                "äº‹ä»¶": "#E2F0CB", "æ¦‚å¿µ": "#B5EAD7", "æŠ€æœ¯": "#C7CEEA"
            }

            # é»˜è®¤é¢œè‰²
            default_color = "#B0B0B0"

            # è·Ÿè¸ªå·²æ·»åŠ çš„èŠ‚ç‚¹
            added_nodes = set()

            # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
            for record in records:
                src = record["source"]
                src_type = record["source_type"]
                tgt = record["target"]
                tgt_type = record["target_type"]
                rel = record["relationship"]
                properties = record.get("rel_properties", {})

                # æ·»åŠ æºèŠ‚ç‚¹ï¼ˆå¦‚æœå°šæœªæ·»åŠ ï¼‰
                if src not in added_nodes:
                    color = type_colors.get(src_type, default_color)
                    net.add_node(src, title=f"{src_type}: {src}", label=src, color=color)
                    added_nodes.add(src)

                # æ·»åŠ ç›®æ ‡èŠ‚ç‚¹ï¼ˆå¦‚æœå°šæœªæ·»åŠ ï¼‰
                if tgt not in added_nodes:
                    color = type_colors.get(tgt_type, default_color)
                    net.add_node(tgt, title=f"{tgt_type}: {tgt}", label=tgt, color=color)
                    added_nodes.add(tgt)

                # æ·»åŠ å…³ç³»è¾¹
                edge_title = f"{rel}"

                # æ·»åŠ å±æ€§ä¿¡æ¯
                if properties:
                    edge_title += "\n" + "\n".join([f"{k}: {v}" for k, v in properties.items()])

                # æ·»åŠ è¾¹
                net.add_edge(src, tgt, title=edge_title, label=rel)

            # ä¿å­˜å¯è§†åŒ–
            net.save_graph(output_file)
            print(f"âœ… çŸ¥è¯†å›¾è°±å¯è§†åŒ–å·²ä¿å­˜è‡³: {output_file}")
            return output_file
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ä¿å­˜å¤±è´¥: {e}")
            return ""

    def generate_narrative(self, records: List[Dict]) -> str:
        """
        å°†çŸ¥è¯†å›¾è°±ç»“æœæ•´åˆæˆè¿è´¯å™è¿°

        å‚æ•°:
            records: çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ

        è¿”å›:
            å™è¿°æ–‡æœ¬
        """
        if not records:
            return "æ ¹æ®çŸ¥è¯†å›¾è°±ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ„å»ºå®ä½“å…³ç³»æ˜ å°„
        entity_relations = {}
        for record in records:
            source = record["source"]
            target = record["target"]
            relationship = record["relationship"]

            # æ’é™¤åµŒå…¥å‘é‡å±æ€§
            properties = {
                k: v for k, v in record.get("rel_properties", {}).items()
                if k != "embedding" and not k.startswith("vector")
            }

            # ä¸ºæºå®ä½“è®°å½•å…³ç³»
            if source not in entity_relations:
                entity_relations[source] = {"outgoing": [], "incoming": []}

            entity_relations[source]["outgoing"].append({
                "target": target,
                "relationship": relationship,
                "properties": properties
            })

            # ä¸ºç›®æ ‡å®ä½“è®°å½•å…³ç³»
            if target not in entity_relations:
                entity_relations[target] = {"outgoing": [], "incoming": []}

            entity_relations[target]["incoming"].append({
                "source": source,
                "relationship": relationship,
                "properties": properties
            })

        # ç”Ÿæˆå™è¿°æ–‡æœ¬
        narrative = "æ ¹æ®çŸ¥è¯†å›¾è°±åˆ†æï¼Œä»¥ä¸‹æ˜¯ç›¸å…³ä¿¡æ¯ï¼š\n\n"

        for entity, relations in entity_relations.items():
            # å®ä½“ä»‹ç»
            narrative += f"â€¢ {entity}ï¼š"

            # å…¥è¾¹å…³ç³»ï¼ˆæŒ‡å‘è¯¥å®ä½“çš„å…³ç³»ï¼‰
            if relations["incoming"]:
                incoming_desc = []
                for rel in relations["incoming"]:
                    desc = f"{rel['source']} {rel['relationship']} {entity}"

                    # æ·»åŠ å±æ€§æè¿°ï¼ˆæœ€å¤šæ˜¾ç¤º2ä¸ªå±æ€§ï¼‰
                    if rel["properties"]:
                        props = ", ".join([f"{k}ä¸º{v}" for i, (k, v) in enumerate(rel["properties"].items()) if i < 2])
                        if len(rel["properties"]) > 2:
                            props += "ç­‰"
                        desc += f" ({props})"

                    incoming_desc.append(desc)

                narrative += "å—åˆ° " + "ã€".join(incoming_desc) + " çš„å½±å“ã€‚"

            # å‡ºè¾¹å…³ç³»ï¼ˆä»è¯¥å®ä½“å‡ºå‘çš„å…³ç³»ï¼‰
            if relations["outgoing"]:
                outgoing_desc = []
                for rel in relations["outgoing"]:
                    desc = f"{entity} {rel['relationship']} {rel['target']}"

                    # æ·»åŠ å±æ€§æè¿°ï¼ˆæœ€å¤šæ˜¾ç¤º2ä¸ªå±æ€§ï¼‰
                    if rel["properties"]:
                        props = ", ".join([f"{k}ä¸º{v}" for i, (k, v) in enumerate(rel["properties"].items()) if i < 2])
                        if len(rel["properties"]) > 2:
                            props += "ç­‰"
                        desc += f" ({props})"

                    outgoing_desc.append(desc)

                # è¿æ¥è¯å¤„ç†
                if relations["incoming"]:
                    narrative += " åŒæ—¶ï¼Œ"

                narrative += "æ¶‰åŠ " + "ã€".join(outgoing_desc) + "ã€‚"

            narrative += "\n"

        return narrative

    def query_whole_graph(self, limit: int = 500) -> Dict:
        """
        æŸ¥è¯¢æ•´ä¸ªçŸ¥è¯†å›¾è°±ï¼ˆé™åˆ¶æ•°é‡ï¼‰
        ä¿®æ”¹ id() ä¸º elementId()
        """
        if not self.driver:
            return {"nodes": [], "links": []}

        try:
            with self.driver.session() as session:
                # æ„å»ºæŸ¥è¯¢è¯­å¥ï¼ˆæŸ¥è¯¢æ‰€æœ‰å…³ç³»ï¼Œé™åˆ¶æ•°é‡ï¼‰- ä¿®æ”¹ id() ä¸º elementId()
                query = """
                        MATCH (start)-[r]->(end)
                        RETURN start.name AS source, 
                               labels(start)[0] AS source_type,
                               type(r) AS relationship, 
                               end.name AS target,
                               labels(end)[0] AS target_type,
                               elementId(start) AS source_id,
                               elementId(end) AS target_id
                        LIMIT $limit
                        """

                # æ‰§è¡ŒæŸ¥è¯¢
                result = session.run(query, limit=limit)
                records = [dict(record) for record in result]

                # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
                nodes = []
                links = []
                node_set = set()

                for record in records:
                    source_id = record["source_id"]
                    source_name = record["source"]
                    source_type = record["source_type"]
                    target_id = record["target_id"]
                    target_name = record["target"]
                    target_type = record["target_type"]
                    rel_type = record["relationship"]

                    # æ·»åŠ æºèŠ‚ç‚¹
                    if source_id not in node_set:
                        nodes.append({
                            "id": source_id,
                            "name": source_name,
                            "type": source_type
                        })
                        node_set.add(source_id)

                    # æ·»åŠ ç›®æ ‡èŠ‚ç‚¹
                    if target_id not in node_set:
                        nodes.append({
                            "id": target_id,
                            "name": target_name,
                            "type": target_type
                        })
                        node_set.add(target_id)

                    # æ·»åŠ å…³ç³»
                    links.append({
                        "source": source_id,
                        "target": target_id,
                        "type": rel_type
                    })

                return {"nodes": nodes, "links": links}
        except Exception as e:
            print(f"âŒ å›¾æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return {"nodes": [], "links": []}

    def get_kg_statistics(self) -> Dict[str, int]:
        """è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        stats = {"entities": 0, "relationships": 0}
        if not self.driver:
            return stats

        try:
            with self.driver.session() as session:
                # æŸ¥è¯¢å®ä½“æ•°é‡
                result = session.run("MATCH (e) RETURN count(e) as entity_count")
                record = result.single()
                stats["entities"] = record["entity_count"] if record else 0

                # æŸ¥è¯¢å…³ç³»æ•°é‡
                result = session.run("MATCH ()-[r]->() RETURN count(r) as rel_count")
                record = result.single()
                stats["relationships"] = record["rel_count"] if record else 0
        except Exception as e:
            print(f"âŒ è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

        return stats

    def query_whole_graph(self, limit: int = 500) -> Dict:
        """
                æŸ¥è¯¢æ•´ä¸ªçŸ¥è¯†å›¾è°±ï¼ˆé™åˆ¶æ•°é‡ï¼‰

                å‚æ•°:
                    limit: è¿”å›çš„å…³ç³»æ•°é‡é™åˆ¶

                è¿”å›:
                    {"nodes": èŠ‚ç‚¹åˆ—è¡¨, "links": å…³ç³»åˆ—è¡¨}
                """
        if not self.driver:
            return {"nodes": [], "links": []}

        try:
            with self.driver.session() as session:
                # æ„å»ºæŸ¥è¯¢è¯­å¥ï¼ˆæŸ¥è¯¢æ‰€æœ‰å…³ç³»ï¼Œé™åˆ¶æ•°é‡ï¼‰
                query = """
                        MATCH (start)-[r]->(end)
                        RETURN start.name AS source, 
                               labels(start)[0] AS source_type,
                               type(r) AS relationship, 
                               end.name AS target,
                               labels(end)[0] AS target_type,
                               elementId(start) as source_id,
                               elementId(end) as target_id
                        LIMIT $limit
                        """

                # æ‰§è¡ŒæŸ¥è¯¢
                result = session.run(query, limit=limit)
                records = [dict(record) for record in result]

                # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
                nodes = []
                links = []
                node_set = set()

                for record in records:
                    source_id = record["source_id"]
                    source_name = record["source"]
                    source_type = record["source_type"]
                    target_id = record["target_id"]
                    target_name = record["target"]
                    target_type = record["target_type"]
                    rel_type = record["relationship"]

                    # æ·»åŠ æºèŠ‚ç‚¹
                    if source_id not in node_set:
                        nodes.append({
                            "id": source_id,
                            "name": source_name,
                            "type": source_type
                        })
                        node_set.add(source_id)

                    # æ·»åŠ ç›®æ ‡èŠ‚ç‚¹
                    if target_id not in node_set:
                        nodes.append({
                            "id": target_id,
                            "name": target_name,
                            "type": target_type
                        })
                        node_set.add(target_id)

                    # æ·»åŠ å…³ç³»
                    links.append({
                        "source": source_id,
                        "target": target_id,
                        "type": rel_type
                    })

                return {"nodes": nodes, "links": links}
        except Exception as e:
            print(f"âŒ å›¾æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return {"nodes": [], "links": []}

    # åœ¨knowledge_graph.pyçš„KnowledgeGraphManagerç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•

    def get_triples_paginated(self, page: int = 1, per_page: int = 3, search: str = '') -> Dict:
        """åˆ†é¡µè·å–ä¸‰å…ƒç»„æ•°æ®ï¼ˆå¤´èŠ‚ç‚¹-å…³ç³»-å°¾èŠ‚ç‚¹ï¼‰"""
        if not self.driver:
            return {'triples': [], 'total_pages': 0, 'total_items': 0}

        try:
            with self.driver.session() as session:
                # è®¡ç®—è·³è¿‡æ•°é‡
                skip = (page - 1) * per_page

                # æ„å»ºæŸ¥è¯¢ï¼ˆæ”¯æŒæœç´¢ï¼‰
                match_clause = "MATCH (h)-[r]->(t)"
                where_clause = ""
                if search:
                    where_clause = f"WHERE h.name CONTAINS '{search}' OR t.name CONTAINS '{search}' OR type(r) CONTAINS '{search}'"

                # è·å–æ€»æ•°é‡
                count_query = f"{match_clause} {where_clause} RETURN count(*) as total"
                total_result = session.run(count_query)
                total_items = total_result.single()['total']
                total_pages = (total_items + per_page - 1) // per_page

                # è·å–åˆ†é¡µæ•°æ®
                data_query = f"""
                {match_clause} {where_clause}
                RETURN id(h) as head_tid, h.name as head_name, labels(h)[0] as head_type,
                       id(t) as tail_tid, t.name as tail_name, labels(t)[0] as tail_type,
                       id(r) as rid, type(r) as relation_type, properties(r) as relation_props,
                       properties(h) as head_props, properties(t) as tail_props
                ORDER BY head_name, relation_type, tail_name
                SKIP {skip} LIMIT {per_page}
                """

                results = session.run(data_query)
                triples = []

                for record in results:
                    triples.append({
                        'head': {
                            'tid': record['head_tid'],
                            'name': record['head_name'],
                            'type': record['head_type'],
                            'properties': record['head_props'] or {}
                        },
                        'tail': {
                            'tid': record['tail_tid'],
                            'name': record['tail_name'],
                            'type': record['tail_type'],
                            'properties': record['tail_props'] or {}
                        },
                        'relation': {
                            'rid': record['rid'],
                            'type': record['relation_type'],
                            'properties': record['relation_props'] or {}
                        }
                    })

                return {
                    'triples': triples,
                    'total_pages': total_pages,
                    'total_items': total_items
                }
        except Exception as e:
            print(f"åˆ†é¡µè·å–ä¸‰å…ƒç»„å¤±è´¥: {e}")
            return {'triples': [], 'total_pages': 0, 'total_items': 0}

    def get_triple_details(self, head_tid: int, tail_tid: int, rid: int) -> Dict:
        """è·å–å•ä¸ªä¸‰å…ƒç»„çš„è¯¦ç»†ä¿¡æ¯"""
        if not self.driver:
            return {}

        try:
            with self.driver.session() as session:
                query = """
                MATCH (h)-[r]->(t)
                WHERE id(h) = $head_tid AND id(t) = $tail_tid AND id(r) = $rid
                RETURN id(h) as head_tid, h.name as head_name, labels(h)[0] as head_type, properties(h) as head_props,
                       id(t) as tail_tid, t.name as tail_name, labels(t)[0] as tail_type, properties(t) as tail_props,
                       id(r) as rid, type(r) as relation_type, properties(r) as relation_props
                """

                result = session.run(query, head_tid=head_tid, tail_tid=tail_tid, rid=rid)
                record = result.single()

                if record:
                    return {
                        'head': {
                            'tid': record['head_tid'],
                            'name': record['head_name'],
                            'type': record['head_type'],
                            'properties': record['head_props'] or {}
                        },
                        'tail': {
                            'tid': record['tail_tid'],
                            'name': record['tail_name'],
                            'type': record['tail_type'],
                            'properties': record['tail_props'] or {}
                        },
                        'relation': {
                            'rid': record['rid'],
                            'type': record['relation_type'],
                            'properties': record['relation_props'] or {}
                        }
                    }
                return {}
        except Exception as e:
            print(f"è·å–ä¸‰å…ƒç»„è¯¦æƒ…å¤±è´¥: {e}")
            return {}

    def update_triple_properties(self, head_tid: int, tail_tid: int, rid: int, data: Dict) -> bool:
        """æ›´æ–°ä¸‰å…ƒç»„çš„å±æ€§ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹å’Œå…³ç³»çš„å±æ€§ï¼‰"""
        if not self.driver:
            return False

        try:
            with self.driver.session() as session:
                # æ›´æ–°å¤´èŠ‚ç‚¹å±æ€§
                if 'head_properties' in data:
                    session.run("""
                    MATCH (n) WHERE id(n) = $tid
                    SET n += $props
                    """, tid=head_tid, props=data['head_properties'])

                # æ›´æ–°å°¾èŠ‚ç‚¹å±æ€§
                if 'tail_properties' in data:
                    session.run("""
                    MATCH (n) WHERE id(n) = $tid
                    SET n += $props
                    """, tid=tail_tid, props=data['tail_properties'])

                # æ›´æ–°å…³ç³»å±æ€§
                if 'relation_properties' in data:
                    session.run("""
                    MATCH ()-[r]->() WHERE id(r) = $rid
                    SET r += $props
                    """, rid=rid, props=data['relation_properties'])

                return True
        except Exception as e:
            print(f"æ›´æ–°ä¸‰å…ƒç»„å±æ€§å¤±è´¥: {e}")
            return False

    def update_node_properties(self, node_id: int, data: Dict) -> bool:
        """å•ç‹¬æ›´æ–°èŠ‚ç‚¹å±æ€§"""
        if not self.driver:
            return False

        try:
            with self.driver.session() as session:
                session.run("""
                MATCH (n) WHERE id(n) = $nid
                SET n += $props
                """, nid=node_id, props=data.get('properties', {}))
                return True
        except Exception as e:
            print(f"æ›´æ–°èŠ‚ç‚¹å±æ€§å¤±è´¥: {e}")
            return False

    def update_relation_properties(self, rid: int, data: Dict) -> bool:
        """å•ç‹¬æ›´æ–°å…³ç³»å±æ€§"""
        if not self.driver:
            return False

        try:
            with self.driver.session() as session:
                session.run("""
                MATCH ()-[r]->() WHERE id(r) = $rid
                SET r += $props
                """, rid=rid, props=data.get('properties', {}))
                return True
        except Exception as e:
            print(f"æ›´æ–°å…³ç³»å±æ€§å¤±è´¥: {e}")
            return False

    def get_relation_details(self, rid: int) -> dict:
        """
        è·å–å•ä¸ªå…³ç³»çš„è¯¦ç»†ä¿¡æ¯ - ä¿®å¤ç‰ˆ
        ä¿®å¤ï¼šæ·»åŠ ridå­—æ®µï¼Œç»Ÿä¸€è¿”å›æ ¼å¼
        """
        if not self.driver:
            return {'success': False, 'error': 'æ•°æ®åº“æœªè¿æ¥'}

        try:
            with self.driver.session() as session:
                # âœ… åŒæ—¶è¿”å›elementId()å’Œå†…éƒ¨ID
                query = """
                MATCH ()-[r]->()
                WHERE elementId(r) = $rid
                RETURN elementId(r) as element_id, id(r) as internal_id,
                       type(r) as relation_type, properties(r) as relation_props
                """

                result = session.run(query, rid=str(rid))
                record = result.single()

                if record:
                    return {
                        'success': True,
                        'relation': {
                            'rid': str(record['element_id']),  # âœ… ç»Ÿä¸€ä¸ºrid
                            'internal_id': record['internal_id'],
                            'type': record['relation_type'],
                            'properties': record['relation_props'] or {}
                        }
                    }
                return {'success': False, 'error': 'å…³ç³»ä¸å­˜åœ¨'}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def check_connection(self) -> bool:
        """
        æ£€æŸ¥Neo4jè¿æ¥æ˜¯å¦å¥åº·
        :return: è¿æ¥çŠ¶æ€
        """
        if not self.driver:
            return False
        try:
            with self.driver.session() as session:
                session.run("RETURN 1 as ping").single()
            return True
        except:
            return False

    def search_triple(self, head: str = None, relation: str = None, tail: str = None, limit: int = 50) -> Dict:
        """æ ¹æ®å¤´èŠ‚ç‚¹ã€å…³ç³»ã€å°¾èŠ‚ç‚¹ç»„åˆæŸ¥è¯¢ä¸‰å…ƒç»„"""
        if not self.driver:
            return {"nodes": [], "links": []}

        try:
            with self.driver.session() as session:
                # æ„å»ºåŠ¨æ€æŸ¥è¯¢
                conditions = []
                params = {}

                if head:
                    conditions.append("h.name CONTAINS $head")
                    params["head"] = head
                if relation:
                    conditions.append("type(r) CONTAINS $relation")
                    params["relation"] = relation
                if tail:
                    conditions.append("t.name CONTAINS $tail")
                    params["tail"] = tail

                where_clause = ""
                if conditions:
                    where_clause = "WHERE " + " AND ".join(conditions)

                # å…³é”®ä¿®æ”¹ï¼šid() æ”¹ä¸º elementId()
                query = f"""
                        MATCH (h)-[r]->(t)
                        {where_clause}
                        RETURN h.name AS source, labels(h)[0] AS source_type,
                               type(r) AS relationship,
                               t.name AS target, labels(t)[0] AS target_type,
                               elementId(h) AS source_id, elementId(t) AS target_id
                        LIMIT $limit
                        """

                params["limit"] = limit

                result = session.run(query, params)
                records = [dict(record) for record in result]

                # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
                nodes = []
                links = []
                node_set = set()

                for record in records:
                    # æ·»åŠ å¤´èŠ‚ç‚¹
                    if record["source_id"] not in node_set:
                        nodes.append({
                            "id": record["source_id"],
                            "name": record["source"],
                            "type": record["source_type"]
                        })
                        node_set.add(record["source_id"])

                    # æ·»åŠ å°¾èŠ‚ç‚¹
                    if record["target_id"] not in node_set:
                        nodes.append({
                            "id": record["target_id"],
                            "name": record["target"],
                            "type": record["target_type"]
                        })
                        node_set.add(record["target_id"])

                    # æ·»åŠ å…³ç³»
                    links.append({
                        "source": record["source_id"],
                        "target": record["target_id"],
                        "type": record["relationship"]
                    })

                return {"nodes": nodes, "links": links}
        except Exception as e:
            print(f"ä¸‰å…ƒç»„æŸ¥è¯¢å¤±è´¥: {e}")
            return {"nodes": [], "links": []}

    def get_node_details(self, node_id: int) -> dict:
        """
        è·å–å•ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯ - ä¿®å¤ç‰ˆ
        ä¿®å¤ï¼šæ·»åŠ tidå­—æ®µï¼Œå…¼å®¹ä¸åŒæ ¼å¼
        """
        if not self.driver:
            return {'success': False, 'error': 'æ•°æ®åº“æœªè¿æ¥'}

        try:
            with self.driver.session() as session:
                # âœ… åŒæ—¶è¿”å›elementId()å’Œå†…éƒ¨ID
                query = """
                MATCH (n)
                WHERE elementId(n) = $node_id
                RETURN elementId(n) as element_id, id(n) as internal_id, 
                       n.name as name, labels(n)[0] as type, properties(n) as properties
                """

                result = session.run(query, node_id=str(node_id))
                record = result.single()

                if record:
                    return {
                        'success': True,
                        'node': {
                            'tid': str(record['element_id']),  # âœ… ç»Ÿä¸€ä¸ºtid
                            'internal_id': record['internal_id'],  # ä¿ç•™å†…éƒ¨IDå¤‡ç”¨
                            'name': record['name'] or 'æœªå‘½å',
                            'type': record['type'] or 'å®ä½“',
                            'properties': record['properties'] or {}
                        }
                    }
                return {'success': False, 'error': 'èŠ‚ç‚¹ä¸å­˜åœ¨'}
        except Exception as e:
            return {'success': False, 'error': str(e)}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºçŸ¥è¯†å›¾è°±ç®¡ç†å™¨
    print("ğŸ› ï¸ åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨...")
    kg = KnowledgeGraphManager(ann_leaf_size=30)

    # è®¾ç½®ç›¸ä¼¼åº¦å‚æ•°
    kg.similarity_threshold = 0.75  # ç›¸ä¼¼åº¦é˜ˆå€¼
    kg.top_k = 3  # æ¯ä¸ªå®ä½“/å…³ç³»è¿”å›çš„æœ€ç›¸ä¼¼ç»“æœæ•°

    # ç¤ºä¾‹æ–‡æœ¬
    text = """
    é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œè®¡ç®—çš„æ–°å…´æŠ€æœ¯ã€‚IBMå’ŒGoogleæ˜¯é‡å­è®¡ç®—é¢†åŸŸçš„é¢†å…ˆä¼ä¸šã€‚
    """

    # ä¸»è¦æµç¨‹ï¼šå¤„ç†ç”¨æˆ·æŸ¥è¯¢
    print("\nå¤„ç†ç”¨æˆ·æŸ¥è¯¢:")
    kg_results = kg.process_user_query(
        text,
        save_to_db=True,  # æ˜¯å¦ä¿å­˜æå–ç»“æœåˆ°æ•°æ®åº“
        depth=2,  # æŸ¥è¯¢æ·±åº¦
        similarity_threshold=kg.similarity_threshold,  # ç›¸ä¼¼åº¦é˜ˆå€¼
        top_k=kg.top_k  # è¿”å›çš„ç›¸ä¼¼ç»“æœæ•°é‡
    )

    # æ ¼å¼åŒ–ç»“æœ
    print("\næ ¼å¼åŒ–ç»“æœ:")
    formatted = kg.format_kg_results(kg_results)
    print(formatted)

    # ç”Ÿæˆå™è¿°æ–‡æœ¬
    print("\nå™è¿°æ–‡æœ¬:")
    narrative = kg.generate_narrative(kg_results)
    print(narrative)

