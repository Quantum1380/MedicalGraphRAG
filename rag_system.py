#!/usr/bin/env python3
# coding: utf-8

"""
GraphRAGSystem - æ•´åˆå‘é‡æ£€ç´¢ä¸çŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. æ¥æ”¶å¤–æŒ‚æ–‡æ¡£å¹¶æ›´æ–°å‘é‡æ•°æ®åº“
2. å¤„ç†ç”¨æˆ·è¾“å…¥çš„é•¿æ–‡æœ¬æŸ¥è¯¢
3. åŒæ—¶è°ƒç”¨å‘é‡æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±æ£€ç´¢
4. æ•´åˆä¸¤ç§æ£€ç´¢ç»“æœå½¢æˆè§„èŒƒçš„Prompt
5. è°ƒç”¨å¤§æ¨¡å‹APIç”Ÿæˆæœ€ç»ˆå›ç­”
"""
import datetime

# !/usr/bin/env python3
# coding: utf-8
"""
GraphRAGSystem â€“ å¥åº·æ¨é€ä¸“ç”¨ç²¾ç®€ç‰ˆ
åªå¤„ç† **ç”¨æˆ·å¥åº·ç”»åƒ** â†’ å‘é‡åŒ¹é…å­å›¾ â†’ ç”Ÿæˆ **ç»“æ„åŒ–æ¨é€**
"""
import os
import time
import json
from typing import Dict, List, Any, Optional
from openai import OpenAI
from config import config
from knowledge_graph import KnowledgeGraphManager
from vector_db import VectorDBManager
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, UnstructuredMarkdownLoader
)
import glob


class GraphRAGSystem:
    def __init__(self, kg_manager=None, vdb_manager=None):
        self.kg = kg_manager or KnowledgeGraphManager()
        self.vdb = vdb_manager or VectorDBManager()
        self.openai_client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY", config.TONGYI_KEY),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.max_tokens = 1400
        # åœ¨ __init__ é‡Œè¿½åŠ 
        self.max_kg_results = 10
        self.max_vdb_results = 5
        self.max_context_length = 4000

    def query(self, user_input: str, depth: int = 2, similarity_threshold: float = 0.75, top_k: int = 5) -> Dict:
        # 1ï¸âƒ£ å‘é‡æ£€ç´¢
        vdb_res = self.vdb.hybrid_search(user_input, k=top_k) if self.vdb.is_initialized else []

        # 2ï¸âƒ£ çŸ¥è¯†å›¾è°±æ£€ç´¢
        kg_res = self.kg.process_user_query(user_input, save_to_db=False, depth=depth,
                                            similarity_threshold=similarity_threshold, top_k=top_k)

        # 3ï¸âƒ£ è·å–æ¿€æ´»çš„Promptæ¨¡æ¿
        active_prompt = self.get_active_prompt_template()

        if active_prompt:
            # ä½¿ç”¨æ¿€æ´»çš„æ¨¡æ¿
            prompt = active_prompt['content'].format(
                user_input=user_input,
                kg_results=json.dumps(kg_res, ensure_ascii=False, indent=2) if kg_res else "æš‚æ— å›¾è°±åŒ¹é…",
                vdb_results="".join(
                    [f"ç‰‡æ®µ {i + 1}: {doc.page_content[:300]}... [æ¥æº]({doc.metadata.get('source', 'æ— æ¥æº')})\n\n" for
                     i, doc in enumerate(vdb_res)]) if vdb_res else "æš‚æ— å‘é‡åŒ¹é…",
                current_date=datetime.now().strftime("%Y-%m-%d"),
                user_name="ç”¨æˆ·"  # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢
            )
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
            prompt = f"""ä½ æ˜¯ä¸€åå¥åº·çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·åŸºäºä¸‹é¢ç”¨æˆ·çš„å¥åº·ç”»åƒä¸çŸ¥è¯†å›¾è°±ç»“æœï¼Œç”¨ä¸“ä¸šçš„è¯­å¥æ¨é€å‡º**ä¸ªæ€§åŒ–å¥åº·çŸ¥è¯†**ï¼Œè¦æ±‚ï¼š

    - ä»…å›´ç»•ç”¨æˆ· **çœŸå®å¥åº·çŠ¶å†µ** ä¸ **çŸ¥è¯†å›¾è°±ä¸­çš„æœ‰æ•ˆä¿¡æ¯**
    - åˆ†ç‚¹æ¨é€å‡ºæœ‰å…³ç³»çš„å¥åº·çŸ¥è¯†ï¼Œå¹¶ä¸”è¦æ˜ç¡®æ ‡æ³¨æ¯ä¸ªçŸ¥è¯†ç‚¹çš„æ¥æºç½‘å€
    - è¦æ±‚æ¨é€çš„å¥åº·çŸ¥è¯†ä¸ç”¨æˆ·çš„å¥åº·çŠ¶å†µã€çŸ¥è¯†å›¾è°±æœ‰æ•ˆä¿¡æ¯å¼ºç›¸å…³
    - å¯ä»¥ç¨å¾®ç»™å‡ºåœ¨*é¥®é£Ÿã€è¿åŠ¨ã€ç”¨è¯ã€å¤æŸ¥ã€æ³¨æ„äº‹é¡¹ç­‰å…·ä½“å¯æ“ä½œå»ºè®®
    - ä»¥ Markdown æ ¼å¼è¾“å‡ºï¼Œå¯å«å°æ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æƒ…ç¬¦å·
    - **é‡è¦**ï¼šåœ¨æ¯ä¸ªçŸ¥è¯†ç‚¹åé¢å¿…é¡»ç”¨ [æ¥æº](URL) çš„æ ¼å¼æ ‡æ³¨æ¥æºé“¾æ¥ï¼ŒURL å¿…é¡»å®Œæ•´å¯ç‚¹å‡»

    ---
    ### ğŸ‘¤ ç”¨æˆ·å¥åº·ç”»åƒ
    {user_input}

    ---

    ### ğŸ” çŸ¥è¯†å›¾è°±åŒ¹é…ç»“æœ
    {json.dumps(kg_res, ensure_ascii=False, indent=2) if kg_res else "æš‚æ— å›¾è°±åŒ¹é…"}

    ---

    ### ğŸ“„ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    {"".join([f"ç‰‡æ®µ {i + 1}: {doc.page_content[:300]}... [æ¥æº]({doc.metadata.get('source', 'æ— æ¥æº')})"+os.linesep for i, doc in enumerate(vdb_res)]) if vdb_res else "æš‚æ— å‘é‡åŒ¹é…"}

    ---

    è¯·å¼€å§‹ç”Ÿæˆ **ä¸“å±å¥åº·çŸ¥è¯†æ¨é€**ï¼Œç¡®ä¿æ¯ä¸ªçŸ¥è¯†ç‚¹éƒ½æœ‰æ˜ç¡®çš„æ¥æºæ ‡æ³¨ï¼š
    """

        # 4ï¸âƒ£ è°ƒç”¨å¤§æ¨¡å‹
        resp = self.openai_client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system",
                 "content": "ä½ æ˜¯åŒ»å­¦å¥åº·çŸ¥è¯†åŠ©æ‰‹ï¼ŒåŸºäºæ‚£è€…çš„å¥åº·ç”»åƒå’Œé™„åŠ çš„ç›¸å…³ä¿¡æ¯ï¼Œå‘æ‚£è€…è¾“å‡ºç»“æ„åŒ–å¥åº·çŸ¥è¯†ï¼Œè¦æ±‚è¯¦ç»†ã€ç²¾å‡†ï¼Œå¹¶ä¸”å¿…é¡»ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æ ‡æ³¨å®Œæ•´å¯ç‚¹å‡»çš„æ¥æºURLã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=self.max_tokens,
            temperature=0.35
        )

        answer = resp.choices[0].message.content.strip()
        return {
            "answer": answer,
            "kg_results": kg_res,
            "vdb_results": [d.page_content[:300] + "..." for d in vdb_res],
            "prompt_used": active_prompt['name'] if active_prompt else "é»˜è®¤æ¨¡æ¿"
        }

    def get_active_prompt_template(self):
        """è·å–å½“å‰æ¿€æ´»çš„Promptæ¨¡æ¿"""
        try:
            import sqlite3
            import os
            from config import config

            conn = sqlite3.connect(config.SQLITE_DB_PATH)
            conn.row_factory = sqlite3.Row
            c = conn.cursor()

            c.execute('SELECT * FROM prompt_templates WHERE is_active = 1')
            active_template = c.fetchone()
            conn.close()

            if active_template:
                return {
                    'id': active_template['id'],
                    'name': active_template['name'],
                    'content': active_template['content']
                }
        except Exception as e:
            print(f"è·å–æ¿€æ´»Promptæ¨¡æ¿å¤±è´¥: {str(e)}")

        return None

    def update_knowledge_base(self, file_pattern: str) -> bool:
        """
        æ›´æ–°çŸ¥è¯†åº“ï¼šåŠ è½½å¤–éƒ¨æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¹¶æå–çŸ¥è¯†åˆ°å›¾è°±

        å‚æ•°:
            file_pattern: æ–‡ä»¶è·¯å¾„æ¨¡å¼ (å¦‚ "documents/*.pdf")

        è¿”å›:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        print(f"ğŸ”„ æ›´æ–°çŸ¥è¯†åº“: {file_pattern}")

        # 1. æ›´æ–°å‘é‡æ•°æ®åº“
        vdb_success = self.vdb.update_from_files(file_pattern)
        if not vdb_success:
            print("âŒ å‘é‡æ•°æ®åº“æ›´æ–°å¤±è´¥")
            return False

        # 2. ä»æ–‡æ¡£ä¸­æå–çŸ¥è¯†åˆ°å›¾è°±
        print("ğŸ“š ä»æ–‡æ¡£ä¸­æå–çŸ¥è¯†åˆ°å›¾è°±...")
        for file_path in glob.glob(file_pattern):
            print(f"  å¤„ç†æ–‡ä»¶: {file_path}")
            try:
                # åŠ è½½æ–‡æ¡£æ–‡æœ¬
                if file_path.endswith('.pdf'):
                    loader = PyPDFLoader(file_path)
                elif file_path.endswith('.docx'):
                    loader = Docx2txtLoader(file_path)
                elif file_path.endswith('.md'):
                    loader = UnstructuredMarkdownLoader(file_path)
                elif file_path.endswith('.txt'):
                    loader = TextLoader(file_path)
                else:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                    continue

                # è·å–æ–‡æ¡£å†…å®¹
                docs = loader.load()
                if not docs:
                    print(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_path}")
                    continue

                # å¤„ç†æ¯ä¸ªæ–‡æ¡£é¡µé¢
                for i, doc in enumerate(docs):
                    text = doc.page_content
                    if not text.strip():
                        continue

                    # ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»
                    extraction_result = self.kg.extract_entities_relations(text)

                    # ä¿å­˜åˆ°çŸ¥è¯†å›¾è°±
                    if extraction_result.get("entities") or extraction_result.get("relationships"):
                        self.kg.save_to_neo4j(
                            extraction_result.get("entities", []),
                            extraction_result.get("relationships", [])
                        )
                        print(f"  é¡µé¢ {i + 1}: æå–åˆ° {len(extraction_result.get('entities', []))} å®ä½“, "
                              f"{len(extraction_result.get('relationships', []))} å…³ç³»")

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")

        print("âœ… çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")
        return True

    def generate_query_prompt(self, user_query: str, kg_results: List[Dict], vdb_results: List[Any]) -> str:
        """
        ç”Ÿæˆå¤§æ¨¡å‹æŸ¥è¯¢çš„Promptï¼Œæ•´åˆçŸ¥è¯†å›¾è°±å’Œå‘é‡æ£€ç´¢ç»“æœ

        å‚æ•°:
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            kg_results: çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ
            vdb_results: å‘é‡æ•°æ®åº“æ£€ç´¢ç»“æœ

        è¿”å›:
            æ•´åˆåçš„Promptæ–‡æœ¬
        """
        # 1. æ„å»ºç³»ç»Ÿæç¤º
        prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œæ‹¥æœ‰ä¸¤ä¸ªçŸ¥è¯†æ¥æºï¼š
1. çŸ¥è¯†å›¾è°±ï¼šåŒ…å«ç»“æ„åŒ–å®ä½“å’Œå…³ç³»
2. å‘é‡æ£€ç´¢ï¼šåŒ…å«ç›¸å…³æ–‡æ¡£ç‰‡æ®µ

è¯·åŸºäºä»¥ä¸‹çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæ³¨æ„ï¼š
- å¦‚æœçŸ¥è¯†å›¾è°±å’Œæ–‡æ¡£å†…å®¹å†²çªï¼Œä»¥çŸ¥è¯†å›¾è°±ä¸ºå‡†
- å¯¹äºäº‹å®æ€§é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†å›¾è°±
- å¯¹äºå¼€æ”¾æ€§é—®é¢˜ï¼Œå‚è€ƒæ–‡æ¡£å†…å®¹
- å¦‚æœæ— æ³•ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯´æ˜åŸå› 
"""

        # 2. æ·»åŠ çŸ¥è¯†å›¾è°±ç»“æœ
        if kg_results:
            prompt += "\n\n## çŸ¥è¯†å›¾è°±ç»“æœ:\n"

            # é™åˆ¶ç»“æœæ•°é‡
            kg_results = kg_results[:self.max_kg_results]

            # æŒ‰å…³ç³»åˆ†ç»„
            relation_groups = {}
            for record in kg_results:
                rel_type = record["relationship"]
                if rel_type not in relation_groups:
                    relation_groups[rel_type] = []

                # æ·»åŠ å…³ç³»æè¿°
                relation_desc = f"{record['source']}({record['source_type']}) â†’ {record['target']}({record['target_type']})"
                relation_groups[rel_type].append(relation_desc)

            # æ„å»ºçŸ¥è¯†å›¾è°±æè¿°
            for rel_type, items in relation_groups.items():
                if len(items) == 1:
                    prompt += f"- {items[0]} ä¹‹é—´å­˜åœ¨ {rel_type} å…³ç³»\n"
                else:
                    # æå–æ‰€æœ‰æºå®ä½“
                    sources = set([item.split('â†’')[0].strip() for item in items])
                    # æå–æ‰€æœ‰ç›®æ ‡å®ä½“
                    targets = set([item.split('â†’')[1].strip() for item in items])

                    source_list = ", ".join(sources)
                    target_list = ", ".join(targets)
                    prompt += f"- {source_list} ä¸ {target_list} ä¹‹é—´å­˜åœ¨ {rel_type} å…³ç³»\n"
        else:
            prompt += "\n\n## çŸ¥è¯†å›¾è°±ç»“æœ: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\n"

        # 3. æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
        if vdb_results:
            prompt += "\n\n## ç›¸å…³æ–‡æ¡£ç‰‡æ®µ:\n"
            vdb_results = vdb_results[:self.max_vdb_results]
            for i, doc in enumerate(vdb_results):
                source = doc.metadata.get('source', 'æœªçŸ¥')
                url = source if source.startswith('http') else f"file://{source}"
                content = doc.page_content.strip()
                if len(content) > 500:
                    content = content[:250] + " ... " + content[-250:]
                prompt += f"\n**ç‰‡æ®µ {i+1}**  \n{content}  \n"
                prompt += f"ğŸ“ [æŸ¥çœ‹åŸæ–‡]({url})  \n"
        else:
            prompt += "\n\n## ç›¸å…³æ–‡æ¡£ç‰‡æ®µ: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\n"

        # 4. æ·»åŠ ç”¨æˆ·æŸ¥è¯¢
        prompt += f"\n\n## ç”¨æˆ·é—®é¢˜:\n{user_query}\n\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ï¼š"

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        if len(prompt) > self.max_context_length:
            prompt = prompt[:self.max_context_length]

        return prompt

    def query(self, user_input: str, depth: int = 2,
              similarity_threshold: float = 0.7, top_k: int = 5) -> Dict:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›æ•´åˆç»“æœ

        å‚æ•°:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            depth: çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ·±åº¦
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼ç»“æœæ•°é‡

        è¿”å›:
            åŒ…å«å®Œæ•´å“åº”çš„å­—å…¸
        """
        start_time = time.time()
        print(f"\nğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_input[:50]}...")

        # 1. å‘é‡æ•°æ®åº“æ£€ç´¢
        vdb_start = time.time()
        try:
            vdb_results = self.vdb.hybrid_search(user_input, k=top_k * 2)
            print(f"  å‘é‡æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(vdb_results)} ä¸ªç›¸å…³ç‰‡æ®µ, è€—æ—¶ {time.time() - vdb_start:.2f}s")
        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
            vdb_results = []
        print(f"å‘é‡åº“æ£€ç´¢ç»“æœä¸ºï¼š{vdb_results.__str__()}")

        # 2. çŸ¥è¯†å›¾è°±æ£€ç´¢
        kg_start = time.time()
        try:
            kg_results = self.kg.process_user_query(
                user_input,
                save_to_db=False,
                depth=depth,
                similarity_threshold=similarity_threshold,
                top_k=top_k
            )
            print(f"  çŸ¥è¯†å›¾è°±æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(kg_results)} æ¡å…³ç³», è€—æ—¶ {time.time() - kg_start:.2f}s")
        except Exception as e:
            print(f"âŒ çŸ¥è¯†å›¾è°±æ£€ç´¢å¤±è´¥: {str(e)}")
            kg_results = []
        print(f"çŸ¥è¯†å›¾è°±æ£€ç´¢ç»“æœæ•´åˆä¸ºï¼š{kg_results.__str__()}")

        # 3. ç”Ÿæˆæ•´åˆPrompt
        prompt = self.generate_query_prompt(user_input, kg_results, vdb_results)
        print(f"å‘é‡åº“å’ŒçŸ¥è¯†å›¾è°±æ£€ç´¢ç»“æœæ•´åˆä¸ºï¼š{prompt}")

        # 4. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå›ç­”
        llm_start = time.time()
        try:
            print("ğŸ§  è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
            response = self.openai_client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1024
            )

            # è·å–æ¨¡å‹å›ç­”
            answer = response.choices[0].message.content
            print(f"  å¤§æ¨¡å‹å“åº”å®Œæˆ, è€—æ—¶ {time.time() - llm_start:.2f}s")
        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            answer = "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"

        # 5. æ„å»ºå“åº”ç»“æœ
        total_time = time.time() - start_time
        print(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆ, æ€»è€—æ—¶: {total_time:.2f}s")

        return {
            "user_query": user_input,
            "answer": answer,
            "kg_results": kg_results[:self.max_kg_results],
            "vdb_results": [doc.page_content[:500] + "..." for doc in vdb_results[:self.max_vdb_results]],
            "processing_time": total_time,
            "prompt": prompt
        }


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨GraphRAGç³»ç»Ÿ...")

    # 1. åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨
    print("ğŸ› ï¸ åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨...")
    kg_manager = KnowledgeGraphManager(ann_leaf_size=30)

    # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†å™¨
    print("ğŸ› ï¸ åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†å™¨...")
    vdb_manager = VectorDBManager()

    # 3. åˆ›å»ºGraphRAGç³»ç»Ÿ
    print("ğŸ› ï¸ åˆ›å»ºGraphRAGç³»ç»Ÿ...")
    graph_rag = GraphRAGSystem(kg_manager, vdb_manager)

    # 5. ç¤ºä¾‹æŸ¥è¯¢
    queries = [
        "æ‚£è€…å› å­£èŠ‚æ€§èŠ±ç²‰è¿‡æ•å°±è¯Šï¼Œç—‡çŠ¶åŒ…æ‹¬æ‰“å–·åšã€æµæ¶•"
    ]

    # å¤„ç†æ¯ä¸ªæŸ¥è¯¢
    for query in queries:
        print("\n" + "=" * 50)
        print(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: {query}")

        # æ‰§è¡ŒæŸ¥è¯¢
        response = graph_rag.query(
            user_input=query,
            depth=2,  # çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ·±åº¦
            similarity_threshold=0.7,  # ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k=3  # è¿”å›çš„ç›¸ä¼¼ç»“æœæ•°é‡
        )

        # æ‰“å°ç»“æœ
        print("\nğŸ’¡ æœ€ç»ˆå›ç­”:")
        print(response["answer"])
